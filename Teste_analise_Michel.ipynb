{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Teste_analise_Michel.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNXCzYBrzEwR4iszxCv20K3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michelgoncalves/projects/blob/master/Teste_analise_Michel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e6E_lJvU0Bf"
      },
      "source": [
        "## Cenário\n",
        "\n",
        "No seguinte cenário você é a pessoa analista por trás do projeto de data ops junto a uma grande indústria norte-americana.\n",
        "\n",
        "Os dados a serem ingeridos e analisados em nossa plataforma de Big Data são dados de compras (orders), pessoas (people) e devoluções (returns).\n",
        "\n",
        "Sua primeira tarefa é implementar uma aplicação de integração de dados, de modo a assegurar os aspectos de integridade e escalabilidade da solução, garantindo que os dados vão ser importados de maneira adequada para as camadas de consumo.\n",
        "\n",
        "## Entregáveis\n",
        "\n",
        "O primeiro entregável desse cenário deve ser um relatório relatando algumas das anomalias encontradas e investigações possíveis (falamos que aqui encorajamos gente curiosa, certo?!). Para feitura desse relatório vocês podem utilizar quaisquer ferramentas de modelagem e (ou) visualização.\n",
        "\n",
        "O dataset a ser utilizado nesse cenário (.zip com arquivo CSV) você encontra <a href=\"https://drive.google.com/file/d/1a8UCbzXFbqTQi0x8tqCXPRTlB--E7o8I/view?usp=sharing\">aqui</a>.\n",
        "\n",
        "Temos um apreço muito grande por qualidade e disponibilidade. Sendo assim, é bom contarmos com métricas para nos previnir e alertar sobre quaisquer problemas bem como metrificar e monitorar as arquitetura proposta. Logo, apreciamos se você conseguir entregar testes que mensurem a qualidade dos dados junto à sua solução desse primeiro entregável. \n",
        "\n",
        "O segundo entregável consiste na transformação de dados disponíveis em <a href=\"https://drive.google.com/file/d/1IDCjpDZh5St97jw4K_bAewJ8hf-rax9C/view?usp=sharing\">arquivo Json</a> para o formato de dataframe, algo comum no dia a dia da empresa. Após transformar esse Json em dataframe é possível perceber que a coluna \"item_list\" está como dicionário. Seu gestor pediu dois pontos de atenção nessa tarefa:\n",
        "\n",
        "- Expandir a coluna num mesmo dataframe;\n",
        "- Normalizar os itens dessa coluna de dicionário e dividí-los em dois dataframes separados, seguindo o modelo relacional.\n",
        "\n",
        "Imagine que o Json das notas fiscais é disponibilizado em uma API. Como você utilizaria as tecnologias da AWS para ingerir, transformar e, eventualmente, carregar esses dados em um banco de dados Redshift? O terceiro entregável consiste na construção de uma arquitetura de ingestão dos dados de nota fiscal do entregável anterior (como visto <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2020/06/22/Screen-Shot-2020-06-19-at-15.00.38.png\">aqui</a>), a qual deve atender aos seguintes pontos:\n",
        "\n",
        "- Esquemas de fluxo de dados;\n",
        "- Descrições de funcionamento (se necessário);\n",
        "- Nomes de tecnologias em ecossistema AWS (serviços, conectores, bibliotecas e módulos).\n",
        "- Será apreciado como esforço extra se você conseguir avançar mais na aplicação além desse ponto.\n",
        "\n",
        "Já o quarto entregável pode estar contido como comentários em suas soluções prévias, queremos entender melhor como foi seu processo de solução de problemas, quais as hipóteses levantadas e, se tivesse mais tempo, como você poderia melhorar a implementação proposta.\n",
        "\n",
        "Ou seja, temos quatro entregáveis:\n",
        "\n",
        "- Relatório com exemplos de anomalias encontradas e possibilidades dentro da sua experiência e com relação aos dados da base;\n",
        "- Resolução de problema de transformação de dados (NF-e);\n",
        "- Arquitetura exemplo da ingestão anterior (ecossistema AWS);\n",
        "- Comentários a respeito da implementações propostas e melhorias (desenvolvimento incremental).\n",
        "\n",
        "## Dados\n",
        "\n",
        "| Table            | Total Rows | Total Columns                                              |\n",
        "| -----------------|:--------:  | :---------------------------------------------------------:|\n",
        "| Orders           | 9994       | 21                                                         |\n",
        "| People           | 4          | 2                                                          |\n",
        "| Returns          | 296        | 2                                                          |\n",
        "\n",
        "## O que será avaliado?\n",
        "\n",
        "1. Buscamos soluções bem definidas e baseadas em método: soube mostrar quais as hipóteses levantadas? Precisou ao menos de modo resumido o por que escolheu determinado caminho? Quais os prós e contras usados para se basear essa solução e quais os passos para implementá-la?\n",
        "2. Qualidades dos entregáveis, tanto o report de anaomalias encontradas quanto a arquitetura proposta.\n",
        "3. A eficiência do método utilizado para a verificação de anomalias.\n",
        "4. Se tivesse mais tempo, o que você faria para melhorar a sua solução?\n",
        "\n",
        "## \n",
        "\n",
        "“Perception is strong and sight weak. In strategy it is important to see distant things as if they were close and to take a distanced view of close things.”\n",
        "\n",
        "Miyamoto Musashi. Japanese martial artist, philosopher, strategist, writer, artist (1584-1645).\n",
        "\n",
        "がんばろう\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAYHUUYy0u_0"
      },
      "source": [
        "## 1 - Análise da base de dados em CSV\n",
        "Após uma rápida inspeção do arquivo, segui salvando-o em formato .xlsl. \n",
        "Em seguida, procedi em criar um novo relatório do Power BI (ferramenta que tenho maior vivência) e importei o xlsl como fonte de dados. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRj0-74W4l0Z"
      },
      "source": [
        "De anomalias e incongruências que pude perceber, inicio com a existência da tabela `People` que possui somente 4 entradas com nomes de pessoas e regiões. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T36VKc5E48OB"
      },
      "source": [
        "<img src=\"https://github.com/michelgoncalves/projects/blob/master/files_to_download/Entrega1_tabela_People.jpg?raw=true\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olhu3Bzn_6M1"
      },
      "source": [
        "Considerando a importância da tabela de pessoas, optei em criá-la como uma dimensão a partir de extração da tabela principal `Orders`, excluindo valores duplicados, incluindo uma chave primária `Customer ID` e acrescentando atributos coerentes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogdOppDeAYwt"
      },
      "source": [
        "<img src=\"https://github.com/michelgoncalves/projects/blob/master/files_to_download/Entrega1_dim_customers.jpg?raw=true\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTtrN4Rw8EAx"
      },
      "source": [
        "Além disso, a tabela `Returns` apresentava somente os pedidos devolvidos, sem uma relação direta com os demais dados. \n",
        "\n",
        "Minha solução, via PowerQuery foi extrair todos os pedidos da tabela `orders` e concatenar as entradas com a tabela `Returns`, criando a tabela `Order_Returns`, contendo todos os pedidos e classificando-os como `Yes` ou `No` conforme o status de devoluções."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-9WmQmN8qMk"
      },
      "source": [
        "<img src=\"https://github.com/michelgoncalves/projects/blob/master/files_to_download/Entrega1_order_returns.jpg?raw=true\" width = \"20%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCMShsZa5rYj"
      },
      "source": [
        "Com base na tabela principal `Orders` utilizei o PowerQuery para criar dimensões coerentes, que facilitassem o desenvolvimento de análises. Nessse contexto, a tabela `orders` ficou como a principal tabela fato: contento informações sobre pedidos, datas, valores e demais eventos, além das chaves para fixar o relacionamento com as dimensões criadas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mik8sH9E6GOQ"
      },
      "source": [
        "<img src=\"https://github.com/michelgoncalves/projects/blob/master/files_to_download/Entrega1_relacionamento.jpg?raw=true\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd9LJV0QAyG7"
      },
      "source": [
        "A título de exemplo de como as análises tornaram-se mais fáceis de serem realizadas após esse tratamento, temos aqui o comparativo de quantidade e também percentual dos pedidos devolvidos X pedidos realizados. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4rbWlZpBFYj"
      },
      "source": [
        "<img src=\"https://github.com/michelgoncalves/projects/blob/master/files_to_download/Entrega2_orders_returned.jpg?raw=true\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvKMPuIatnHV"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUNRd0RHu9fs"
      },
      "source": [
        "## 2 - Análise do arquivo Json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "460zPeqMvQMy"
      },
      "source": [
        "Para leitura do arquivo Json, utilizei inicialmente a função `pd.read_json` do pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUhKSj1gNRfY"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCYCUJsGNahk"
      },
      "source": [
        "df = pd.read_json('data.json')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "RCveEYefPDx6",
        "outputId": "afb40aae-2bf6-4159-d443-9ad8cf905818"
      },
      "source": [
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CreateDate</th>\n",
              "      <th>EmissionDate</th>\n",
              "      <th>Discount</th>\n",
              "      <th>NFeNumber</th>\n",
              "      <th>NFeID</th>\n",
              "      <th>ItemList</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>501</td>\n",
              "      <td>1</td>\n",
              "      <td>[{'ProductName': 'Rice', 'Value': 35.55, 'Quan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>502</td>\n",
              "      <td>2</td>\n",
              "      <td>[{'ProductName': 'Tomate', 'Value': 12.25, 'Qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>503</td>\n",
              "      <td>3</td>\n",
              "      <td>[{'ProductName': 'Beer', 'Value': 9.0, 'Quanti...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               CreateDate  ...                                           ItemList\n",
              "0  2021-05-24T20:21:34.79  ...  [{'ProductName': 'Rice', 'Value': 35.55, 'Quan...\n",
              "1  2021-05-24T20:21:34.79  ...  [{'ProductName': 'Tomate', 'Value': 12.25, 'Qu...\n",
              "2  2021-05-24T20:21:34.79  ...  [{'ProductName': 'Beer', 'Value': 9.0, 'Quanti...\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSxDFtP5wIaG"
      },
      "source": [
        "Apesar do dataframe ter sido criado, a coluna `ItemList` permaneceu aglutinada em formato de dicionário. \n",
        "\n",
        "Para \"descompactar\" essa coluna, optei em importar a biblioteca `json` e utilizar a função `json.loads`. Após carregado, utilizei a função pandas `json_normalize` e especifiquei a coluna `ItemList` para ser \"quebrada\" do formato dicionário."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ1DMuNWPQAy"
      },
      "source": [
        "import json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_hxT-g3Rdh2"
      },
      "source": [
        "with open('data.json','r') as data_dict:\n",
        "    data = json.loads(data_dict.read())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G1DtCHlUFsT"
      },
      "source": [
        "df = pd.json_normalize(data, record_path =['ItemList'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "KXBsJnfjfqJ7",
        "outputId": "63403242-2b68-46f0-b364-c9267b91877d"
      },
      "source": [
        "df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ProductName</th>\n",
              "      <th>Value</th>\n",
              "      <th>Quantity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rice</td>\n",
              "      <td>35.55</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Flour</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bean</td>\n",
              "      <td>27.15</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tomate</td>\n",
              "      <td>12.25</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pasta</td>\n",
              "      <td>7.55</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Beer</td>\n",
              "      <td>9.00</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>French fries</td>\n",
              "      <td>10.99</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Ice cream</td>\n",
              "      <td>27.15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ProductName  Value  Quantity\n",
              "0          Rice  35.55         2\n",
              "1         Flour  11.55         5\n",
              "2          Bean  27.15         7\n",
              "3        Tomate  12.25        10\n",
              "4         Pasta   7.55         5\n",
              "5          Beer   9.00         6\n",
              "6  French fries  10.99         2\n",
              "7     Ice cream  27.15         1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13IzGt94xuLA"
      },
      "source": [
        "Com a `ItemList` agora devidamente formatada, o próximo passo foi incorporar todas as colunas do json em um só *dataframe*. Para isso, utilizei novamente a fução `json_normalize`, agora não só especificando a coluna a ser achatada, como também referenciando as demais colunas a serem carregadas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkbQwbx_et36"
      },
      "source": [
        "df = pd.json_normalize(\n",
        "    data, \n",
        "    record_path =['ItemList'], \n",
        "    meta=['CreateDate', 'EmissionDate', 'Discount', 'NFeNumber', 'NFeID']\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "DBp6jZkoaSSz",
        "outputId": "bb8e6464-f01f-4e88-9518-cccc61d4ac32"
      },
      "source": [
        "df"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ProductName</th>\n",
              "      <th>Value</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>CreateDate</th>\n",
              "      <th>EmissionDate</th>\n",
              "      <th>Discount</th>\n",
              "      <th>NFeNumber</th>\n",
              "      <th>NFeID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rice</td>\n",
              "      <td>35.55</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>501</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Flour</td>\n",
              "      <td>11.55</td>\n",
              "      <td>5</td>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>501</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bean</td>\n",
              "      <td>27.15</td>\n",
              "      <td>7</td>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>501</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tomate</td>\n",
              "      <td>12.25</td>\n",
              "      <td>10</td>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>502</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pasta</td>\n",
              "      <td>7.55</td>\n",
              "      <td>5</td>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>502</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Beer</td>\n",
              "      <td>9.00</td>\n",
              "      <td>6</td>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>503</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>French fries</td>\n",
              "      <td>10.99</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>503</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Ice cream</td>\n",
              "      <td>27.15</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-05-24T20:21:34.79</td>\n",
              "      <td>2021-05-24T00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>503</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ProductName  Value  Quantity  ... Discount NFeNumber NFeID\n",
              "0          Rice  35.55         2  ...        0       501     1\n",
              "1         Flour  11.55         5  ...        0       501     1\n",
              "2          Bean  27.15         7  ...        0       501     1\n",
              "3        Tomate  12.25        10  ...        0       502     2\n",
              "4         Pasta   7.55         5  ...        0       502     2\n",
              "5          Beer   9.00         6  ...        0       503     3\n",
              "6  French fries  10.99         2  ...        0       503     3\n",
              "7     Ice cream  27.15         1  ...        0       503     3\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS9bWP7rzcnz"
      },
      "source": [
        "Com o *dataframe* unificado e as colunas todas \"descompactadas\", utilizei uma função do Google Colab para convertê-lo em csv, para em seguida levá-lo ao ambiente do Power BI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8X24iRYQ2jIh",
        "outputId": "1eaca133-d83e-4473-cad4-6fdcb8b193ce"
      },
      "source": [
        "df.to_csv('sample.csv' , index=False, header=True)\n",
        "from google.colab import files\n",
        "files.download(\"sample.csv\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d2a60935-0fe6-4775-b575-9fdde3c2e2fd\", \"sample.csv\", 619)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u404T0R1LRB"
      },
      "source": [
        "No Power BI, Iniciei o processo de quebrar o dataframe em duas partes, buscando assim criar um modelo de dados relacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aha8o2R728cu"
      },
      "source": [
        "<img src=\"https://github.com/michelgoncalves/projects/blob/master/files_to_download/Entrega2_fragmented2.jpg?raw=true\" width=\"90%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smZ-1bk52_v9"
      },
      "source": [
        "Separei os dataframes em duas tabelas, uma de dimensão constando os detalhes dos produtos `Dim_product`, e uma fato com o registros das vendas `F_Sale`. Tomei a liberdade de criar uma coluna de valores únicos para funcionar como código dos produtos `ProductCode` e dividi os valores do valor de venda pela quantidade de produtos para criar a coluna de preço `Price`. \n",
        "\n",
        "Para melhor ilustrar ambas tabelas, plotei-as na área de visualização do PowerBI: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnPNfD_f4s8j"
      },
      "source": [
        "<img src=\"https://github.com/michelgoncalves/projects/blob/master/files_to_download/Entrega2_tabelas_finais.jpg?raw=true\" width=\"80%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mGy8YaS6ANu"
      },
      "source": [
        "E aqui uma screenshot de como ficou o relacionamento das duas tabelas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ETQ54aK5eXB"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"https://github.com/michelgoncalves/projects/blob/master/files_to_download/Entrega2_relacionamento.jpg?raw=true\" width=\"80%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndxe8XR04_MP"
      },
      "source": [
        "Nesse formato, programei para que cada entrada de produto, tivesse o seu preço unitário `Price` automaticamente multiplicado pela quantidade `Quantity`, resultando no que pude entender como valor da venda `SaleValue`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aabt6iXGtj_s"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObCB6qoHtY2B"
      },
      "source": [
        "## 3 - Ecossistema AWS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVFPhR35tuvQ"
      },
      "source": [
        "Até o presente momento, busquei solucionar as questões anteriores utilizando minhas habilidades presentes: daí a utilização integrada do Python e PowerBi/PowerQuery(M).\n",
        "\n",
        "Sendo um profissional relativamente novo no mercado de dados, pouco me envolvi até o momento com AWS, sendo que o máximo de contato com *RedShift* foi simplesmente para realizar algumas consultas e extrações via SQL (DBeaver).\n",
        "\n",
        "Dado o disclaimer, considerando o cenário da utilização de soluções ETL  para melhor ingestão dos dados do Json em um servidor *RedShift*, acredito que a utilização do **AWS Glue** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdvOV189uvgq"
      },
      "source": [
        "<img src =\"https://github.com/michelgoncalves/projects/blob/master/files_to_download/AWS_GLue__1_400x260.png?raw=true\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhlgM6CWvOih"
      },
      "source": [
        "Por ser gerenciado pela AWS, o Glue tem algumas origens e scripts pré-definidos, já apropriados para o carregamento dos dados no *RedShift*, que também está no guarda-chuva da Amazon.\n",
        "\n",
        "Seja no formato Json, Csv, etc, o **AWS Glue** utiliza crawlers para captura dos dados e identificação dos mesmos. \n",
        "\n",
        "Uma vez realizada a extração das fontes que preciso, no ambiente do **AWS Glue** terei os dados no formato de tabela e, seja utilizando scripts Python pré-definidos, editando-os ou criando os meus do zero, posso realizar os tratamentos necessários e definir como vou querer que os dados sejam carregados no destino -  *RedShift*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Q65wZDwFHh"
      },
      "source": [
        "<img src =\"https://github.com/michelgoncalves/projects/blob/master/files_to_download/S3SpendwithGlueRedshift2.png?raw=true\" width = \"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qConvtKcwGCF"
      },
      "source": [
        " \n",
        "Desse ponto em diante, com os dados apropriadamente carregados no RedShift, posso tanto realizar uma conexão direta com o Power BI (ou alguma ferramenta similar) para realizar análises mais aprofundadas, gerar visuais, dashboards, reports, etc, que gerem insights importantes ou cumpram objetivos específicos de um projeto. "
      ]
    }
  ]
}